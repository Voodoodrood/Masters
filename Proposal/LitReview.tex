\documentclass{sig-alternate-05-2015}
\usepackage{float}
%\usepackage{enumitem}
\usepackage{subfiles}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{url}
\usepackage{xcolor}
\usepackage{soul}

\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\begin{document}
\title{A user interface for landscape modelling in a VE using a head mounted display}

\numberofauthors{1}
\author{
\alignauthor
Timothy Gwynn\\
       \affaddr{GWYTIM001}\\
       \affaddr{University of Cape Town\\
       Supervisor: J. Gain}
}
\maketitle
\begin{CCSXML}
 	<ccs2012>
 	<concept>
 	<concept_id>10003120.10003123.10010860.10010858</concept_id>
 	<concept_desc>Human-centered computing~User interface design</concept_desc>
 	<concept_significance>500</concept_significance>
 	</concept>
 	<concept>
 	<concept_id>10003120.10003123.10010860.10010859</concept_id>
 	<concept_desc>Human-centered computing~User centered design</concept_desc>
 	<concept_significance>300</concept_significance>
 	</concept>
 	<concept>
 	<concept_id>10010147.10010371.10010387.10010866</concept_id>
 	<concept_desc>Computing methodologies~Virtual reality</concept_desc>
 	<concept_significance>500</concept_significance>
 	</concept>
 	<concept>
 	<concept_id>10010147.10010371.10010396</concept_id>
 	<concept_desc>Computing methodologies~Shape modeling</concept_desc>
 	<concept_significance>100</concept_significance>
 	</concept>
 	</ccs2012>
\end{CCSXML}
 
 
  \ccsdesc[500]{Human-centered computing~User interface design}
  \ccsdesc[300]{Human-centered computing~User centered design}
  \ccsdesc[500]{Computing methodologies~Virtual reality}
  \ccsdesc[100]{Computing methodologies~Shape modeling}

\printccsdesc
\keywords{Virtual Environments, Terrain Synthesis, User Testing}
\begin{abstract}
	Abstract here
\end{abstract}
\section{Introduction}
\section{Background}
\subsection{Terrain Synthesis}
Virtual landscapes are an important component in representing natural environments for applications such as computer games, film, simulation and training\cite{Gain2015}. There has therefore been, considerable research in the area focusing on both realistic terrain synthesis and effective methods for allowing users to model terrains according to given constraints.
\subsubsection{Modelling methods}
Gain et al.'s terrain synthesis system allows users to introduce constraints into the terrain synthesis process using a number of tools\cite{Gain2015}. The ability to add and modify constraint points and curves to a landscape allows users to create landscape features such as hills, ridges and valleys. Type constraints can be painted onto the landscape to define areas of a certain type such as `swamp', `dirt', etc. To implement these constraints users have access to a combination of sketching, painting and 3D widget interface elements. 

Another terrain editing system presents a sketch based technique which allows users to  edit terrain from first person point of view\cite{Tasse2014}. Users are able to sketch a horizon shape which the system then uses to create a number of constraints which the synthesis process must obey.
\subsubsection{Synthesis methods}
 Fast and realistic terrain synthesis has been the subject of considerable study. This has resulted in a variety of techniques such as patch or texture-based synthesis\cite{Cruz2015, Tasse2012}, noise-based synthesis\cite{Musgrave1989} and erosion simulation\cite{Anh2007} being studied. With advances in graphics processing technology we are now able to simulate terrain that is indistinguishable from real terrain examples\cite{Gain2015}. 

\subsection{Large Scale VE interface design}
When designing an interface for modelling in large-scale VEs there are three important aspects to consider: User navigation and movement, Interface interaction and Environment interaction\cite{Bowman2001}. Respectively, these allow the user to move within the VE, select their mode of interaction and interact with the VE. Below we look at each of these aspects in detail and some of the successful, and unsuccessful, interface implementations in each area.
\subsubsection{Navigation}
A common problem for users in VEs is becoming disorientated and lost within the VE\cite{Darken1993}. This may be due to a number of issues such as the user being unfamiliar with the environment or not having sufficient location and orientation information available. Below there are some examples from research that aims to solve the problem of effective navigation and travel in VEs.

Landmarks are commonly used in everyday scenarios that require navigation. As such there has been considerable research into effectively incorporating landmarks into VEs.

One such study investigated whether users orientate themselves via local or global landmarks\cite{Steck2000}.It was found that users do not consistently use a single type of landmark. However, users rarely used both local and global types simultaneously. This suggests that users tend to use the most visually distinct land marks and will prefer to use landmarks that are not occluded. A good example of this would be a static sun-like object in the sky. This implies that for navigation, interface design should provide a variety of landmarks, both global and local, that allow the user to always have a distinctive point of reference. It also suggests that global landmarks that will not be occluded by local geometry should be used. 

Further research into the design of landmarks for VEs found that in natural environments users tend to use landmarks that resemble man-made artefacts\cite{Vinson1999}. Additionally it was suggested that landmarks should all be orientated the same way and marked according to this orientation. This allows the user to extract orientation information as well as location information from a single landmark.

Another study compared a variety of tools for navigation in VEs including landmarks, breadcrumb markers and maps, amongst others\cite{Darken1993}. They associated their techniques with natural human and avian  navigational behaviours. They found that the map view in particular allowed for straightforward navigation although they suggested that this was linked to the navigation space being a 2D plane. Where users could fly vertically, letting them observe the navigational space from above, they were also able to follow a simple sequence of actions for accurate navigation. They also found that, while landmarks were effective for distinguishing certain areas they provided little directional information. When they added a synthetic sun and shadows in the test case with landmarks they found that user performance increased significantly. While they did not evaluate the effectiveness of the different techniques they did detail the common behaviour of the test subjects for each technique. By observing which techniques lead to simple or complex behaviour we can surmise which techniques are effective from an ease of use perspective.

These results reinforce the concept of a combination of local and global landmarks being useful for navigation\cite{Darken1993}. Additionally, the ease of use of both the map and the flying technique suggest that allowing the user to move in 3 dimensions and providing a map will aid navigation considerably. We suggest that in a 3D environment where 3D movement is possible a 3D map is better suited to aiding navigation. This will allow the user to read their height directly from the map and possibly aid the user in determining the scale of objects.

Although having effective navigation techniques is important they are useless without some method of travel. Bowman et al. have created a list of aims for effective travel techniques.\cite{Bowman1997} These include, speed, accuracy, spatial awareness, ease of learning, ease of use, information gathering and presence. We suggest that user comfort should also be taken into account especially when designing a VE interface for 6-DOF controllers.

In the same paper travel techniques are categorized into two groups: Gaze directed steering and gesture directed steering\cite{Bowman1997}. Gaze directed steering refers to instances where the user's navigation is controlled via a tracked HMD while gesture directed  steering involves the user controlling their direction of movement through gestures (with or without props).Their findings suggest that gesture directed steering would be more appropriate in our context according to the aims stated above.

Bowman et al. then performed a series of experiments where they found that tool directed navigation was faster than gaze directed navigation for navigating relative to objects and equal for navigating to an absolute point in space. Additionally since gesture directed steering allows the user to freely observe their environment during travel it allows for superior spatial awareness and information gathering. We therefore suggest that we should prefer gesture directed steering techniques.

\subsubsection{Interface Interaction}
Interface interaction refers to the user modifying the state of the system or the mode of interaction\cite{Bowman2001}. Typical examples of this include menu interaction and tool selection. These tasks, which are often 2D or 1D in nature are ill suited to 3D environments \cite{Bowman2001, Hand1997}. In particular, the increased dimensionality of the task can lead to a significantly higher error rate and issues of occlusion need to be considered\cite{Hand1997}.

Therefore we must consider novel interface elements. For example, a ``ring menu" is a way to implement 1D interface interactions that allow users to navigate menus or select tools\cite{Hand1997}. Other alternatives to traditional menu interaction include gesture-based shortcuts\cite{Zeleznik2007} or speech input \cite{VanDam1997,Bowman2001,Hand1997}. Additionally, combinations of interaction methods can be used. For example, gesture-based shortcuts and radial menus can be combined to allow users utilize muscle memory to select tools from the radial menu using gestures without actually displaying it.\cite{Kurtenbach1993}

Another element of interface interaction to be considered is ``clutching", where a user must pause movement tracking so as to reset to a comfortable position\cite{Hand1997}. Importantly, this needs to be done without interfering with the current interaction. For example, when a user intends to rotate an object further than is comfortable with a single movement. Typically this can be done by providing the user with a button to toggle motion tracking on and off. However, this is annoying for the user and feels unnatural\cite{Hand1997}.  A suggested solution is to utilize a direct mapping of controls onto virtual objects combined with two-handed-interaction(THI). This can reduce or remove the need for artificial clutching mechanisms\cite{Hand1997}.  Hinckley et al. suggest that utilizing THI can reduce the need for clutching by tracking gestures of the dominant hand (DH) relative to those of the non-dominant hand (NDH)\cite{Hinckley1994}. They also suggest that an alternative is to use a physical 3D prop such as those used by Ware et al. for there eye-in-hand metaphor\cite{Ware1990}.

The depth positioning of interface elements is another challenge in VR that is simply not considered in desktop systems.\cite{alger2015visual} This is because HMD devices use stereoscopy to create depth, traditional desktop systems do not have this issue as all interface elements must be placed on the 2D screen. Therefore, we need to avoid having interface elements that are too close to the user in the visual field, so that they are hard to focus on or having elements that are out of comfortable reach. These constraints combine to leave a somewhat restricted area in which to place interface elements.

As well as considering where interface elements are placed in the virtual space we also should provide users with indications of where objects in their physical environment are. This helps reconcile the differences between the virtual reality a user is immersed in and their surroundings\cite{Duval2014}. Reasons to show some of the user's physical environment include: allowing them to more easily interact with physical devices such as keyboards and other objects, preventing inadvertent physical contact which may cause injury or distraction and being able to interact with other people without exiting the VE completely.
\subsubsection{Environment Interaction}
Interaction with the environment involves the user making changes to the virtual world through selection and manipulation of objects. To do this users should, at a minimum, be able to either select, position or rotate objects\cite{Bowman2001}. Research in this area addresses both the wide range of hardware devices created for interacting with VEs as well as related software solutions. Common tools include a motion-tracked glove \cite{Zimmerman1986}, spaceballs\cite{Hand1997}, and pen or wand controllers\cite{Schultheis2012}. As opposed to WIMP interfaces interaction with VEs is often based on direct manipulation where the user 'grabs' an object and then manipulates it as one would manipulate a malleable physical body.

There have also been a number of investigations into the advantages of THI over other interaction methods. It is widely agreed that humans are better able to judge the relative, rather than absolute, position of their hands \cite{Bowman1998, Buxton1986}. Additionally, users have a preference for THI\cite{Buxton1986} and perform certain basic manipulation tasks such as rotation and translation more efficiently when using two hands\cite{Schultheis2012,Balakrishnan1999}. Therefore interfaces should be designed for bi-manual interaction and motion should be tracked with the DH relative to the NDH\cite{Hinckley1994}.

Another area of interest in environment interaction is how to interact with objects far away from the user. This is commonly referred to as action at a distance (AAAD). This would be necessary for our interface to allow users to manipulate points in the terrain that are far away from the users viewpoint. Techniques to address this include the go-go hand technique\cite{Poupyrev1996}, where the hand is tracked and then non-linearly mapped into the VE and a number of ray-casting techniques \cite{Bowman2001}. These methods roughly correspond to the function of a cursor in a 2D environment. Hand et al.\cite{Hand1997} suggest that although cursors are a necessary metaphor in a 2D environment they do not correspond well with natural gestures for use in VEs.

Although there are a number of ways to perform AAAD users also need depth cues to manipulate or select objects with precision.\cite{Schultheis2012} Some suggested methods for this include representing the user's hand as a transparent outline in the VE\cite{Hinckley1994}, drawing a line between the user's hands in the VE\cite{Schultheis2012} or creating a plane of action\cite{Mine2014} on which the user acts. To further simplify environment interaction, particularly when working with stereoscopic depth, studies suggest that interactive DOF should be restricted as much as possible\cite{Bowman2001}.
\section{Related work}
\subsection{Designing for VEs within VEs}
It is only in recent years that the technology for effectively creating content in VE systems has become available. However, there are already a variety of systems for modelling 3D objects and environments in VEs. Additionally, early research into the design of VE systems revealed some basic principles that can be incorporated into modern systems. By looking at solutions in existing systems and incorporating general VE design principles we can address some of the common problems inherent to working with VEs.

From early on in research into THI we found that people have an instinctive awareness of the relative position of their hands\cite{Bowman1998, Buxton1986}. Some modern systems take advantage of this to solve the issue of 2D menu depth in a scene. To do this systems are designed such that the menu is attached to the user's NDH either as a physical touch screen\cite{Wang2013,Mine2014} or as a virtual representation\cite{Jerald2013}. Because the menu is always at a constant position relative to the user's NDH they are able to reliably interact with it using their DH.

Although THI systems that use 6-DOF devices have been shown to increase performance with regards to interaction speed\cite{Schultheis2012} they often induce fatigue in users. For example, in an experiment where a handlebar metaphor was used to control the camera users consistently reported feeling fatigued\cite{Song2012}. However, from previous experiments we observe that bringing geometry to the user instead of making the user reach out to the geometry provides an experience with very few fatigue issues\cite{Jerald2013}.

Another problem related to using 6-DOF devices is how to input textual and numerical data. To address this, a number of systems incorporate a form of voice input to make up for the absence of a keyboard\cite{Ponto2013,Toma2012}. An implementation of this in the \textit{Placeholder} system\cite{Laurel1994} allows users to create audio notes at certain locations that would be recorded and then could be played back at any time.

Modern VE systems that incorporate THI for 3D modelling suggest keeping the functions of each controller relatively independent\cite{Mine2014}. For example, using the NDH to control movement while the DH is used for interacting with the environment. This means that users do not have to use both controllers for every action. This seems to contradict other studies, that show users favour using two hands when able to\cite{Buxton1986,Hinckley1994}. However, by separating the tasks  users are able to simultaneously perform two independent actions.  In the same system having a small number of buttons dedicated to common tasks was found to be effective\cite{Mine2014}.
\subsubsection{CAD based systems}
\subsubsection{Terrain and landscape modelling}
\subsection{Comparison of Desktop and VE applications}
The primary motivation for designers to adopt a VE based workflow is to achieve increased productivity. Therefore, it is important to consider areas in which VE systems perform significantly better than traditional desktop systems.Fortunately, there have been a number of studies comparing user performance in VEs and desktop environments.
\subsubsection{Object Manipulation}
VE systems have been shown to be particularly effective for simple object manipulation. Studies have shown that expert users are able to complete simple tasks up to 8 times faster in the VE than on a desktop\cite{Schultheis2012}.

Some of the most detailed data regarding task performance in VEs comes from an experiment comparing a traditional WIMP interface to a number of VE set-ups. Each VE system tested featured THI with 6-DOF controllers. However, the researchers varied the presence of stereo-vision, haptic feedback and a snapping tool\cite{Scali2003}. Users performed a series of tasks that involved translating, rotating and scaling objects to fit onto pre-defined surfaces. They found that the VE systems outperformed the WIMP interfaces with regards to time for task completion, perceived workload and usability. Additionally they found that although haptics and stereo vision did not significantly affect task completion times, haptics had a significant effect on perceived workload and usability.
\subsubsection{Navigation}
However, in more complex applications, especially where indoor navigation was required, users tended to perform better on desktop systems\cite{SousaSantos2009}. Additionally, from a number of previous experiments it is evident that the amount of training users receive has a significant impact on results. However, even users with little training report that the VE interfaces were more intuitive and easier to use than desktop systems, despite there being no significant difference in task performance.\cite{Toma2012}.
\section{Discussion}
\section{Conclusion}
\bibliographystyle{abbrv}
\bibliography{proposal}
\end{document}