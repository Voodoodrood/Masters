\documentclass{sig-alternate-05-2015}
\usepackage{float}
\usepackage{enumitem}
\usepackage{subfiles}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{tabularx}

\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\begin{document}
\title{A user interface for landscape modelling in a VE using a head mounted display}

\numberofauthors{1}
\author{
\alignauthor
Timothy Gwynn\\
       \affaddr{GWYTIM001}\\
       \affaddr{University of Cape Town}
}
\maketitle
\begin{CCSXML}

\end{CCSXML}



\printccsdesc
\keywords{Virtual Environments, Terrain Synthesis, User Testing}
\begin{abstract}
	
\end{abstract}
\section{Introduction}
Landscape modelling is a method for creating realistic virtual landscapes. It allows the designer to set global and local constraints in order to shape the terrain as desired. The system then creates a simulated terrain that is realistic but also fits within the constraints. In this way the designer can create a realistic terrain with global and local features without manually drawing everything. Virtual terrains are used as components for natural environments in computer games, film, simulation and training\cite{Gain2015}

Virtual environments(VEs) describe a set of three dimensional visual output technologies. These technologies aim to embed users visually within virtual worlds much as technologies such as surround sound or headphones do via audio. Examples of these technologies include CAVE (CAVE Automatic Virtual
Environment)  systems and HMD (head mounted display) devices.
CAVE devices surround the user with a number of large screens as well as tracking user head position\cite{Cruz-Neira1993} This provides the visual equivalent of surround sound. HMD devices fit over the user's face and the device itself is tracked and can be described as headphones for your eyes.\cite{Alger2015}

VE technologies are also often associated with novel input devices such as 6 degree of freedom(DOF) devices which allow users to provide 3 dimensions of positional input and 3 dimensions of rotational input. Thanks to the continuing research into this relatively new technology there a wide range of such devices. Unlike traditional WIMP interfaces where almost every system is designed for a mouse and keyboard VE systems tend to rely on specialized input devices\cite{Hand1997,Bowman2001}. However, with the recent increase in consumer access to commercial devices such as the Oculus Rift and Vive that come with generic controllers for use with multiple games and applications this may be changing.

Creating effective user interfaces for VE technology, especially for the design process allows us to take full advantage of this new technology. Post-WIMP interfaces promise a radical change in the way we interact with these new technologies and, if implemented well, could result in greatly increased productivity and control in VEs.
 
 By creating systems for designers to perform landscape modelling for VEs to be carried out in VEs we allow developers to better understand and design for the end user experience. Additionally, we hope to provide a design environment that is more efficient than a desktop system for experienced users. Due to the association of increased immersion and flow in VEs compared to desktops [need citation] we also hope to encourage these phenomena in the design environment.
\section{Background}
There are large bodies of work on both interactive terrain synthesis and VE interface design. It should be noted that work related to terrain synthesis tends to focus on the synthesis techniques rather than the interface. Additionally, as suggested in the introduction much of the work on VE interfaces has been done using custom or specialized controllers and other hardware. Earlier works in particular discuss issues such as motion sickness which modern hardware has mostly solved. Here we will discuss the more relevant aspects of previous works related to our research. We detail current solutions to interactive terrain synthesis and a range of interface design techniques. We also cover work that has shown that certain tasks can be accomplished more quickly in VEs than on desktop systems. This provides one of our primary motivations for our research. We discuss previous systems that were designed to do similar VE modelling tasks. 
\subsection{Terrain Synthesis} 
Virtual landscapes are an important component in representing natural environments for applications such as computer games, film, simulation and training\cite{Gain2015}. The subject of fast and realistic terrain synthesis has been the subject of considerable study with a variety of techniques such as patch or texture based \cite{Cruz2015, Tasse2012}, noise synthesis\cite{Musgrave1989} and erosion simulation\cite{Anh2007}. With advances in graphics processing technology we are able to simulate terrain that is indistinguishable from real terrain examples\cite{Gain2015}. 

 Gain et al.'s terrain synthesis system allows users to introduce constraints into the terrain synthesis process using a number of tools\cite{Gain2015}. The ability to add and modify geometric constraint points and curves to a landscape allows users to create landscape features such as hills, ridges and valleys.  Coherence controls allow users to fix parts of the landscape preventing later changes elsewhere from affecting it. Copy/paste functionality allows landscape regions to be moved to new locations and elevations. Type constraints which can be painted onto the landscape to define areas of a certain type such as 'swamp', 'dirt' etc. To interact with these tools users have access to a combination of sketching, painting and 3D widget interface elements. As this software will form the basis for the application being created in this study we will need to reproduce and modify these tools for use in VEs.

Additionally, we will need to take advantage of a first person view (FPV) state for editing the terrain from a FPV. Tasse et al.'s paper on first person terrain editing presents a sketch based technique which allows users to  edit terrain from a single point of view\cite{Tasse2014}. Also of note is that, although there is focus on editing the terrain from a FPV, the users requested the ability to have a separate viewpoint from which to see their changes made in the FPV. In the paper itself we are regularly given a top view to display how the changes affected the terrain as a whole. This reinforces the need to give users both an immersive FPV and a remote top-view.
\subsection{Interface design for modelling in large-scale Virtual environments}
Somewhere need to talk about camera control, not sure where cite ware and bowman
\subsubsection{Navigation in Virtual Environments}
There are a number of studies analyzing techniques for user navigation and viewpoint control in VEs.

Bowman et al. have created a list of aims for effective travel techniques.\cite{Bowman1997} These include, speed, accuracy, spatial awareness, ease of learning, ease of use, information gathering and presence. We suggest that user comfort should also be taken into account especially when designing a VE interface for 6DOF controllers.

 In the same paper Bowman et al. present a categorization of travel techniques\cite{Bowman1997}. This allows us to categorize a number of techniques into two broad categories: Gaze directed steering and gesture directed steering. Gaze directed steering refers to instances where the user's navigation is controlled via a tracked HMD while gesture directed  steering involves the user controlling their direction of movement through gestures (with or without props). Bowman et al. then performed a series of experiments where they found that tool directed navigation was faster than gaze directed navigation for navigating relative to objects and equal for navigating to an absolute point in space. Additionally since gesture directed steering allows the user to freely observe their environment during travel it allows for superior spatial awareness and information gathering. We therefore suggest that we should prefer gesture directed steering techniques.

Other areas of research related to navigation include investigation into the effectiveness of various types of landmarks. These include a study of whether users orientate themselves via local or global landmarks\cite{Steck2000}. Steck et al. found that although users would alternate between using local and global landmarks in certain situations they tended not to combine them. They suggest that users will tend to use the most visually distinct land marks and will prefer to use landmarks that are not occluded. This implies that navigation interface design should provide a variety of landmarks, both global and local, that allow the user to always have a distinctive point of reference. It also suggests that global landmarks that will not be occluded by local geometry should be used. A good example of this would be a static sun-like object in the sky.

Vinson et al. performed further research into the design of landmarks for VEs.\cite{Vinson1999} They found that users tend to use landmarks that resemble manmade artifacts in natural environments. Additionally they suggest that landmarks should all be orientated the same way and marked accoriding to this orientation. This allows the user to extract orientation information as well as location information from a single landmark.

Darken et al. compared a variety of tools for navigation in VEs including landmarks, breadcrumb markers and map views amongst others\cite{Darken1993}. They associated their techniques with natural human and avian  navigational behaviours. While they did not evaluate the effectiveness of the different techniques they did detail the common behaviour of the test subjects for each technique. By observing which techniques lead to simple or complex behaviour we can surmise which techniques are effective from at least an ease of use perspective. They found that the map view in particular allowed for very simple behaviour although they suggested that this was linked to the navigation space being a 2D plane. Users also were able to follow a simple sequence of actions in a scenario where they were able to fly vertically which let them observe the navigational space from above at a distant perspective. They also found that while landmarks were effective for distinguishing certain areas they provided little directional information. When they added a synthetic sun in the test case with landmarks which provided orientation information user performance increased significantly.

Darken et al.'s experiments reinforce the concept of a combination of local and global landmarks being useful for navigation.\cite{Darken1993} Additionally, the ease of use of both the map and the flying technique suggest that allowing the user to move in 3 dimensions and providing a 3D map will aid navigation considerably. 
\subsubsection{Interface interaction in VEs}
 Interaction with the interface refers to the user modifying the state of the system or the mode of interaction\cite{Bowman2001}. Typical examples include menu interaction and tool selection. These tasks, which are often 2D or 1D are ill suited to 3D environments\cite{Bowman2001, Hand1997}.
 
 Hand et al. also observe that directly converting 2D interface elements to a stereoscopic system can cause difficulties.\cite{Hand1997} Particularly, the increased dimensionality of the task can lead to a significantly higher error rate and issues of occlusion need to be taken into account.
 
 Hand et al. suggest a "ring menu" as a way to implement 1 dimensional interface interactions that allow users to navigate menus or select tools.\cite{Hand1997} Some alternatives to menu interaction at all include gesture based shortcuts such as those used by Zeleznik et al.\cite{Zeleznik2007} or speech input as suggested by multiple papers. \cite{VanDam1997,Bowman2001,Hand1997} An interesting combination of the gesture based shortcuts and radial menu concepts is letting users utilize miuscle memory to select tools from the radial menu without actually displaying it.\cite{Kurtenbach1993}
 
 The issue of "clutching", where a user must pause movement tracking so as to reset to a comfortable position without interfering with the current interaction is also mentioned in Hand et al.'s paper.\cite{Hand1997} An example of this is when a user desires to rotate an object further than is comfortable with a single movement. It is suggested that utilizing direct mapping of controls onto virtual objects combined with THI can reduce or remove the need for this.\cite{Hand1997}  Hinckley et al. suggest that utilizing THI can reduce the need for clutching by tracking gestures of the dominant hand relative to those of the non-dominant hand.\cite{Hinckley1994} They also suggest that an alternative is to use a physical 3D prop such as those used by [find paper where world in miniature is used with camera prop]
 
 One more aspect of interface interaction design that has been explored is the positioning of interface elements.\cite{alger2015visual} Here we need to avoid issues where the interface is too close to the user so that it is hard to focus on. However, interface elements must not be out of comfortable reach. These constraints combine to leave a relatively restricted area in which we can place interface elements.
 
 [Talk about visualizing physical enviroment or aspects therof] 
 
 Mine et al. created custom 3D controllers using a combination of touch screens and physical buttons.\cite{Mine2014} These were well suited for menu selections as the 2D touch surface mapped directly to 2D interface elements. From this we can surmise that finding an alternative to motion tracking for interface interaction is advisable. [Can either discuss other specific techniques for interface interaction here or move it to related work]
 
 
\subsubsection{Environment interaction in VEs}
What to put here: [Widget design and interaction(go go hand, ray-casting), painting and sketching, user comfort in design, cursors or intersection lines, depth issues, trying to handle 2D objects or points, natural design, audio notes]


 Interaction with environment involves the user making changes to the virtual environment through selection and manipulation of objects. To do this users should, at minimum, be able to either select or position or rotate objects\cite{Bowman2001}. Research in this area addresses both the wide range of hardware devices created for interacting the VEs as well as the related software solutions. Common tools include a motion tracked glove \cite{Zimmerman1986} and pen or wand controllers.\cite{Schultheis2012}
 
 There have also been a number of investigations into the advantages of THIs over other interaction methods. It is widely agreed that humans are better able to judge relative position of their hands than absolute position.\cite{Bowman1998, Buxton1986} Additionally users have a preference for THI\cite{Buxton1986} and perform certain basic manipulation tasks such as rotation and translation more efficiently when using two hands.\cite{Schultheis2012,Balakrishnan1999} Therefore interfaces should be designed for bi-manual interaction and motion should be tracked with the dominant hand(DH) relative to the non-dominant hand (NDH).\cite{Hinckley1994}
 
 Another area of interest in Environment interaction is how to interaction with objects far away from the user. This is commonly referred to as action at a distance (AAAD). Techniques to address this include the go-go hand technique\cite{Poupyrev1996}, where the hand is tracked and then non-linearly mapped into the VE and a number of ray-casting techniques discussed by Bowman et al.\cite{Bowman2001}. These methods roughly correspond to the function of a cursor in a 2D environment. Hand et al. suggest that although cursors are a necessary metaphor in a 2D environment they do not correspond well enough with natural gestures for use in VEs.\cite{Hand1997}
 
 Although there are a number of ways to perform AAAD users also need depth cues to manipulate or select objects with precision.\cite{Schultheis2012} Some suggested methods for this include representing the user's hand as a transparent outline in the VE\cite{Hinckley1994} or drawing a line between the user's hands in the VE.\cite{Schultheis2012}
 
 
 Studies also suggest that degrees of freedom (DOF) should be restricted as much as possible to simplify interaction with the VE.\cite{Bowman2001}
 
 
  With regards to the software solutions a number of 3D widgets haven been implemented and tested [examples and citations here].
 

\subsection{Usability testing}
\subsection{Comparison of Desktop applications to Virtual environment}
There have been a number of studies comparing user performance in VEs and desktop environments. In studies where users had to perform simple tasks expert users completed the tasks almost 8 times faster in the VE than on a desktop.\cite{Schultheis2012} However, in more complex applications, especially where indoor navigation was required or training time was low, users tended to perform better on desktop systems\cite{SousaSantos2009}. However, even in systems where users were novices they felt that the VE interfaces were more intuitive and easier to use\cite{Toma2012}.

 Scali et al. performed an experiment comparing a traditional WIMP interface to a number of VE set-ups with 6-DOF controllers that varied stereo-vision, haptic feedback and a snapping tool\cite{Scali2003}. They found that the VE systems outperformed the WIMP interfaces with regards to time for task completion, perceived workload and usability. Additionally they found that although haptics and stereo vision did not significantly affect task completion times haptics had a significant effect on perceived workload and usability.
\subsection{Designing for Virtual Environments in Virtual Environments}
There a large variety of systems that have been developed to study methods of designing 3D objects and environments in VEs. These systems use a variety of devices and techniques to overcome some of the problems with VE design such as menu interaction.

A common technique for supporting menu interaction is to take advantage of users innate knowledge of the relative positions of their hands\cite{Bowman1998, Buxton1986}. To do this systems are designed such that the menu is attached to the user's NDH either as a physical touch screen\cite{Wang2013,Mine2014} or as a virtual representation\cite{Jerald2013}

Another result from previous experiments is that being able to bring the geometry to the user provides an experience with very few fatigue issues\cite{Jerald2013}. Fatigue is something we need to keep in mind when designing the system as Song et al. found when trying to implement a handle bar metaphor for visual control\cite{Song2012}.

The problem of inputting text for VE environments while using 6-DOF devices is certainly one of the more pressing issues for the technology. A number of systems incorporate an aspect of voice input to make up for this loss\cite{Ponto2013,Toma2012}. Of particular interest is a method used in the system "Placeholder" created by Laurel et al. that allowed users to create audio notes at certain locations that could be recorded and then played back at any time\cite{Laurel1994}.

In their paper Mine et al. make a number of suggestions on how to effectively map certain actions in the virtual world to two hand controllers\cite{Mine2014}. Although they used custom controllers some of the concepts are general enough to carry over to a range of THI. Particularly they suggest keeping the functions of each hand relatively independent such that users do not have to use both controllers at once. This seems to contradict what a number of studies that have shown that users are capable of and in fact tend to use two hands when able to\cite{Buxton1986,Hinckley1994}. What separating the tasks may have done in Mine et al.'s experiment is allow users to simultaneously perform two independent tasks.  Mine et al. also suggest having a small number of buttons dedicated to essential tasks that are performed often\cite{Mine2014}.
\section{Research question(s)}
\section{Method}
In order to establish the answers to the research questions above we will implement a system based on Gain et al.'s terrain synthesis system\cite{Gain2015}. This system will use the same terrain synthesis methods but will incorporate a new interface that has been re-designed for use in a VE. We will then be able to compare how users perform on the existing desktop system against performance using the VE system. This will allow us to determine which system performs better on a number of measures detailed later.
\subsection{System Design}
\subsubsection{Hardware and Software requirements}
The system will use a HMD device together with two 6-DOF controllers to provide output and allow for user input.The user will interact with the system using only these devices. The system will be designed such that it can be used while seated at a desk. This will help reduce user fatigue\cite{Schultheis2012}. There will also need to be real-time or close to real-time feedback based on users input as the existing desktop system currently does.

The interface will be created using Unity 3D in Windows while the back end software from the existing desktop system will remain unchanged unless absolutely necessary.

\subsubsection{User-System interaction}
The design will be based on the findings of previous work in the area of designing for VEs in VEs. This will lead to a VE system that is optimized for both work flow speed and usability as well as user experience. We will aim to maximise user comfort while allowing for terrain modelling to be done quickly and accurately. The system will feature controls that allow for the sketching, painting and widget manipulation present in the existing desktop system. We will be able to implement each of these features independently although the system will need the sketching and widget manipulation features at a minimum to function meaningfully. We will provide a menu system that the user can navigate using either the joysticks on the controllers or through gesture recognition.

We will also need to implement at least one method for user navigation and camera control such that the user can view and interact with the environment. Ideally we hope that the user will be able to switch between a first person viewing mode where they are able to move over the landscape and a remote, top down view where the entire terrain or large sections thereof will be visible.

\subsection{Experimental design}
We will need two groups of users to perform the experiment. The control group will use the existing desktop system while the experimental group will use the VE system. Each group will attend two sessions. During the first session both groups will perform a series of training exercises. It has been shown that training has a significant effect on user performance in VEs\cite{Schultheis2012}. During this session users will be able to familiarise themselves with the system they are using. The second session will occur a short number of days later. This will prevent the effectiveness of the training sessions from fading significantly. The two sessions will not occur immediately after one another to prevent fatigue from training with unfamiliar systems from affecting the results of the experiment.

During the second session each group will be asked to perform the same terrain modelling tasks such as replicating an existing terrain as closely as possible. Users will be monitored for speed and accuracy. Additionally, the experiments will be followed by questionnaires regarding the usability of the system as well as questions related to VE issues such as fatigue, motion sickness and disorientation.

Our independent variable will be the system used (Desktop or VE) and our independent variables will be speed, accuracy and user satisfaction/ usability.
\subsection{Data collection}
During the experiment we will have a telemetry system running in the background that will provide detailed information on user inputs and actions. We will also be recording the time it takes users to complete each modelling task. After all tasks have been completed we will then collect usability feedback via questionnaire. 
\subsection{Data analysis}
\section{Project plan}
\subsection{Requirements}
\subsection{Risks}
\subsection{Timeline}
\section{Research outcomes}
\subsection{Artifacts produced}
\subsection{Success factors}
\subsection{Relevance to industry}
\subsection{Further research}

\bibliographystyle{abbrv}
\bibliography{proposal}
\end{document}